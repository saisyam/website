<!doctype html>

<html lang="en">

<head>
  <title>Effective Web Scraping with Python - Saisyam</title>
  <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="description" content="Saisyam&#39;s personal blog for technical articles" />
<meta name="author" content="Saisyam" /><meta property="og:title" content="Effective Web Scraping with Python" />
<meta property="og:description" content="Web scraping is a process of automatically extracting information from websites. Web scrapers are small programs written to crawl and extract specific information from a webpage. For example, getting latest news items from news websites like BBC, CNN etc." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://saisyam.com/effective-web-scraping-with-python/" />
<meta property="og:image" content="https://saisyam.com/webscraping.jpg" />
<meta property="article:published_time" content="2021-01-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-01-10T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://saisyam.com/webscraping.jpg"/>

<meta name="twitter:title" content="Effective Web Scraping with Python"/>
<meta name="twitter:description" content="Web scraping is a process of automatically extracting information from websites. Web scrapers are small programs written to crawl and extract specific information from a webpage. For example, getting latest news items from news websites like BBC, CNN etc."/>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-50423294-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<meta name="generator" content="Hugo 0.79.0" />
    

  <link rel="stylesheet" href="https://saisyam.com/css/normalize.min.css" />
  <link rel="stylesheet" href="https://saisyam.com/fontawesome/css/all.min.css" />
  
  
    
    <link rel="stylesheet" href="https://saisyam.com/css/fonts.css" />
  
  
  <link rel="stylesheet" type="text/css" href="https://saisyam.com/css/styles.css" /><style>
    #toc::before {
        content: "Table of Contents";
        font-weight: bold;
    }
    #toc nav {
      border-bottom: none;
      padding-bottom: 0.5em;
      font-family: "Ruda", sans-serif;
    }

    #toc nav ul {
      border: 1px solid #aaa;
      padding: 1.5em;
      list-style: decimal;
      display: inline-block;
    }
  </style>
<link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
<link rel="manifest" href="favicon/site.webmanifest">
<link rel="mask-icon" href="favicon/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>

<body>
  <div id="container">
    <header>
      <h1>
                <a href="https://saisyam.com/">Saisyam</a>
            </h1>

      <ul id="social-media">
             <li>
               <a href="https://www.facebook.com/saisyam.dampuri.5667" title="Facebook">
               <i class="fab fa-facebook fa-lg"></i>
               </a>
             </li>
             <li>
               <a href="https://github.com/saisyam" title="GitHub">
               <i class="fab fa-github fa-lg"></i>
               </a>
             </li>
             <li>
               <a href="https://twitter.com/saisyam1" title="Twitter">
               <i class="fab fa-twitter fa-lg"></i>
               </a>
             </li>
             <li>
               <a href="https://linkedin.com/in/saisyam" title="LinkedIn">
               <i class="fab fa-linkedin fa-lg"></i>
               </a>
             </li>
             <li>
               <a href="https://www.instagram.com/saisyamdampuri" title="Instagram">
               <i class="fab fa-instagram fa-lg"></i>
               </a>
             </li>
      </ul>
      
      <p><em>Personal Blog</em></p>
      
    </header>

    
<nav>
    <ul>
        
    </ul>
</nav>


    <main>




<article>
    <h1>Effective Web Scraping with Python</h1>

    
      <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2021-01-10T00:00:00Z">Jan 10, 2021</time>
        </li>
        
        
        <li>
            Categories:
            <em>
                
                    
                    <a href="https://saisyam.com/categories/python">Python</a>
                
            </em>
        </li>
        

        
        <li>
            <em>
                
                    
                    <a href="https://saisyam.com/tags/web-scraping">#Web Scraping</a>
                
                    , 
                    <a href="https://saisyam.com/tags/splinter">#Splinter</a>
                
                    , 
                    <a href="https://saisyam.com/tags/beautifulsoup">#BeautifulSoup</a>
                
                    , 
                    <a href="https://saisyam.com/tags/requests">#Requests</a>
                
                    , 
                    <a href="https://saisyam.com/tags/proxies">#Proxies</a>
                
                    , 
                    <a href="https://saisyam.com/tags/webdriver">#Webdriver</a>
                
            </em>
        </li>
        

        <li>5 minutes read</li>
    </ul>
</aside>

    

    
<div class="featured_image">
    <a href="https://saisyam.com/effective-web-scraping-with-python/" title="Effective Web Scraping with Python">
        <img src="../webscraping.jpg">
    </a>
</div>



    <p>Web scraping is a process of automatically extracting information from websites. Web scrapers are small programs written to crawl and extract specific information from a webpage. For example, getting latest news items from news websites like BBC, CNN etc.</p>
<div id="toc">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#why-scraping">Why Scraping?</a></li>
    <li><a href="#scraping-with-python">Scraping with Python</a></li>
    <li><a href="#challenges-while-scraping">Challenges while scraping</a></li>
    <li><a href="#python-packages-for-scraping">Python packages for scraping</a></li>
    <li><a href="#scraping-approach">Scraping approach</a></li>
    <li><a href="#using-proxies-for-ip-rotation">Using proxies for IP rotation</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</div>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-0206245742790279"
     data-ad-slot="3890452391"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2 id="why-scraping">Why Scraping?</h2>
<p>There are millions of websites on the Internet which have information on different topics. For example, news, travel, food, finance, business, tech, entertainment, to name a few. All these websites don&rsquo;t have a proper way (APIs or feeds) of sharing data. So, we need scrape such websites to extract the information.</p>
<p>Scrapers parse the HTML elements of the page using tags, ids, class names etc to get the required content from the page. Consider the following simple HTML page:</p>
<script type="application/javascript" src="https://gist.github.com/saisyam/9d13a4e0c16de8326b491bd89a77c963.js"></script>

<p>The above HTML snippet contains tags like, <code>title</code>, <code>div</code>, <code>h1</code>, and <code>p</code>. We can extract the <code>div</code> element with <code>id=main</code> and then extract text between <code>h1</code> and <code>p</code> tags under the <code>div</code> element. We can identify specific tags using <code>id</code> or <code>class</code> or any other attributes.</p>
<h2 id="scraping-with-python">Scraping with Python</h2>
<p>It&rsquo;s easy to write scrapers in Python. Python has a rich set of tools or packages to perform scraping effectively. The below code snippet parses the above HTML and extracts the information required:</p>
<script type="application/javascript" src="https://gist.github.com/saisyam/7d19d9c6247194e9130e578e08be6336.js"></script>

<p>The above code uses <code>Beautifulsoup</code> package. The response of the above function is:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{
	<span style="color:#e6db74">&#39;title&#39;</span>: <span style="color:#e6db74">&#39;Sample HTML page&#39;</span>,
	<span style="color:#e6db74">&#39;heading&#39;</span>: <span style="color:#e6db74">&#39;Scraping with Python&#39;</span>,
	<span style="color:#e6db74">&#39;summary&#39;</span>: <span style="color:#e6db74">&#39;This article explains how web scraping is done in Python and the packages required.&#39;</span>,
	<span style="color:#e6db74">&#39;author&#39;</span>: <span style="color:#e6db74">&#39;By Saisyam&#39;</span>
}
</code></pre></div><h2 id="challenges-while-scraping">Challenges while scraping</h2>
<p>Not all web pages are so simple as shown in the above example. Below is the list of challenges that we encounter while scraping:</p>
<ol>
<li><strong>Dynamic content</strong> - Today most of the websites use latest JavaScript frameworks to build their sites. The content or the HTML that is rendered is generated dynamically with JavaScript. You need a real browser to load the webpage inorder to get the rendered HTML</li>
<li><strong>Getting blocked</strong> - Websites will block your IP address if they see a continuous traffic from you. While developing the scraper you will continuously hit their website which makes the website think of a suspicious activity and block your IP. If possible download the static HTML page and build your scraper. Most scrapers use proxy IP addresses to solve this problem.</li>
<li><strong>Captcha</strong> - Most websites will check whether you are a human or not when they detect a suspicious activity as mentioned in point <strong>#2</strong>. Always try to avoid this situation. Though there are captcha solving tools, they are not 100% reliable or accurate.</li>
</ol>
<h2 id="python-packages-for-scraping">Python packages for scraping</h2>
<p>In order to parse HTML, first we need to get the HTML of the page. There are two ways in Python to get the HTML content of the page:</p>
<ol>
<li><strong>Using <code>requests</code> package</strong> - <a href="https://requests.readthedocs.io/en/master/">Requests</a> is a Python package, HTTP library built for humans. We use <code>requests.get()</code> method to get the HTML content of a page. The advantage of this is, it is fast and simple but will not get the dynamically loaded content as it is not a browser to execute JavaScript.</li>
<li><strong>Using <code>splinter</code> package</strong> - <a href="https://splinter.readthedocs.io/en/latest/">Splinter</a> is a Python package used to test web applications. Splinter uses webdrivers (Chrome, Firefox etc) to load a web page and get HTML. You need to have browser application and associated webdriver to make it work. With Splinter we can get dynamically loaded content as well.</li>
</ol>
<h2 id="scraping-approach">Scraping approach</h2>
<p>The approach that we follow for creating scrapers are:</p>
<ol>
<li>Get HTML content of the website using <code>requests</code> or <code>splinter</code> based on the requirement.</li>
<li>Use <code>Beautifulsoup</code> to parse the HTML and extract the content. BeautifulSoup](<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a>) is a Python package used to parse HTML content. <code>Beautifulsoup</code> supports multiple HTML parsers. <code>html5lib</code> is a parser built completely in Python and considered as the fastest among the existing parsers.</li>
</ol>
<p>Getting the complete HTML content is a complex activity, because of the challenges explained in the above section. We avoid using single IP address and use proxies and rotate our IPs. There are some free proxy providers like <a href="https://sslproxies.org/">SSLProxy</a>, <a href="https://spys.one/en/https-ssl-proxy/">SpysOne</a>, <a href="https://www.proxy-list.download/HTTPS">ProxyList</a> to name a few.</p>
<p>If you don&rsquo;t want to deal with all the mess and do just parsing the content, use <a href="https://www.scraperapi.com/">ScraperAPI</a>. It is a proxy API for web scraping. Scraper API handles proxies, browsers, and CAPTCHAs, so you can get the HTML from any web page with a simple API call!. Check out <a href="https://www.scrapingbee.com/">ScrapingBee</a> as well.</p>
<h2 id="using-proxies-for-ip-rotation">Using proxies for IP rotation</h2>
<p>You can tell <code>requests</code> API to use proxies to fetch html.</p>
<script type="application/javascript" src="https://gist.github.com/saisyam/12410ff3c8c027cf0b58c903bff0d5a8.js"></script>

<p>Your request will go through the proxy address provided. Use HTTP proxies to skip SSL verification step which happens in case of HTTPS proxies.</p>
<p>In a similar way, we can use proxies with Splinter as well. We use Chrome browser here.</p>
<script type="application/javascript" src="https://gist.github.com/saisyam/7b8b7f4954086ae581a59b31731ec38f.js"></script>

<p>If one IP gets blocked, you can get an another one and continue scraping.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this article we learnt what is scraping and why it is required. We have explored few packages in Python to do scraping and identified challenges in scraping along with few solutions to overcome those challenges. For more information on scraping visit my <a href="https://github.com/saisyam/scrapers">Scrapers</a> repository on Github. Happy scraping!</p>


</article>


<section class="post-nav">
    <ul>
        
        <li>
            <a href="https://saisyam.com/reading-and-writing-wav-files-with-compression-in-python-using-pywav/"><i class="fa fa-chevron-circle-left"></i> Reading and Writing WAV files with compression in Python using Pywav</a>
        </li>
        
        
    </ul>
</section>
  
    
    
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "saisyam-1" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    
  


<script type="application/ld+json">
    
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Effective Web Scraping with Python",
      "image": "https://saisyam.com/",
      "datePublished": "2021-01-10T00:00:00Z",
      "dateModified": "2021-01-10T00:00:00Z",
      "author": {
        "@type": "Person",
        "name":  null 
      },
      "mainEntityOfPage": { "@type": "WebPage" },
       "publisher": {
        "@type": "Organization",
        "name":  null ,
        "logo": {
          "@type": "ImageObject",
          "url":  null 
        }
      },
      "description": "Web scraping is a process of automatically extracting information from websites. Web scrapers are small programs written to crawl and extract specific information from a webpage. For example, getting latest news items from news websites like BBC, CNN etc.",
      "keywords": ["Web Scraping", "Splinter", "BeautifulSoup", "Requests", "Proxies", "Webdriver"]
    }
    
    {
      "@context": "https://schema.org",
      "@type": "Organization",
      "name": "Personal blog",
      "url": "https://saisyam.com/",
      "sameAs": [
        "https://www.facebook.com/saisyam.dampuri.5667",
        "https://www.instagram.com/saisyamdampuri",
        "https://twitter.com/saisyam1",
        "https://github.com/saisyam"
      ]
    }
    </script>


</main>
    <footer>
        <h6>Copyright © 2020 - Saisyam |
            Rendered by <a href="https://gohugo.io" title="Hugo">Hugo</a> |
            <a href="https://saisyam.com/index.xml">Subscribe </a></h6>
    </footer>
</div>
<script src="https://saisyam.com/js/scripts.js"></script>


  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-50423294-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



</body>

</html>

